# -*- coding: utf-8 -*-
"""CS334 Final Project Code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbm2sa2oDrqV3RrwUtprWB0BbRqnVQtW
"""

# This code is for the CS 334 Final Project
# Randy Truong and Andrew Yin

# Commented out IPython magic to ensure Python compatibility.
# Import data from Google Drive, on Google Colab
from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive/My Drive/FinalProject.CS334.Fall2020"
# %cd /content/drive/My Drive/FinalProject.CS334.Fall2020

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import time

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import StratifiedShuffleSplit

from sklearn.metrics import accuracy_score

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier

# Take sample from original dataset
# Note: this was done on a local machine
data = pd.read_csv('tweetid_userid_keyword_topics_sentiments_emotions.csv')
print('success!')
print('dataset size', data.shape)

test1 = data.sample(n=1000000)
test1.to_csv('1million.csv', index=False)
print('exported 1!')

# Read data file, convert to dataframe 
data_df = pd.read_csv('1millionnewnew.csv')
data_df.columns = ['tweet_ID', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10',
       'valence_intensity', 'anger_intensity', 'fear_intensity',
       'sadness_intensity', 'joy_intensity', 'sentiment_category',
       'emotion_category', 'keyword_used', 'user_ID']
data_df = data_df.drop(['tweet_ID', 'user_ID', 'keyword_used'], axis=1)

# Plot label distribution for original dataset

data = pd.read_csv('tweetid_userid_keyword_topics_sentiments_emotions.csv')
print('success!')
fig, ax = plt.subplots()
data['emotion_category'].value_counts().plot(ax=ax, kind='bar')
plt.xlabel('emotion category')
plt.ylabel('frequency')
plt.show()

# Plot label distribution for sample
fig, ax = plt.subplots()
plt.xlabel('Emotion Category')
plt.ylabel('Frequency')
data_df['emotion_category'].value_counts().plot(ax=ax, kind='bar')
plt.show()

# FEATURE SELECTION
# Create correlation matrix
corr_matrix = data_df.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.80)]

# Drop features 
new_df = data_df.drop(to_drop, axis=1, inplace=False)
new_df

# SEPARATE DATA AND LABELS
new_data = np.array(new_df)

X = new_data[:, 0:10]
y = new_data[:, 11]

# Encode the labels
le = LabelEncoder()
y = le.fit_transform(y)

print(X)
print(y)

# Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html

# Big data, simple holdout method (data was originally shuffled)
xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.3, random_state=334)

# Scale data
scaler = StandardScaler().fit(xTrain)
xTrain_scaled = scaler.transform(xTrain)
xTest_scaled = scaler.transform(xTest)

# Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html

# Split training data into training and validation set
xTrain_val, xTest_val, yTrain_val, yTest_val = train_test_split(xTrain_scaled, yTrain, test_size = 0.2, random_state=334)

# Decision Tree Classifier
startTime = time.time()
dt = DecisionTreeClassifier(max_depth=13, min_samples_leaf=26, criterion='entropy').fit(xTrain_scaled, yTrain)
dt_preds_test = dt.predict(xTest_scaled)
dt_preds_train = dt.predict(xTrain_scaled)
endTime = time.time()
timeElapsed = endTime - startTime

print(dt_preds)
print(accuracy_score(yTest, dt_preds_test))
print(accuracy_score(yTrain, dt_preds_train))
print('timeElapsed:', timeElapsed)

# Code referenced: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html

# Hyperparameter Tuning for Decision Tree
#parameters = {'criterion': ['gini', 'entropy'], 'max_depth': list(range(5, 50, 5)), 'min_samples_leaf': list(range(5, 50, 5))}
# best params for this: {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20}
# timeElapsed: 277.6822512149811, 1 split
# meanTestScore: 0.9483785714285714

#parameters = {'criterion': ['entropy'], 'max_depth': list(range(11, 20, 2)), 'min_samples_leaf': list(range(11, 27, 3))}
# {'criterion': 'entropy', 'max_depth': 13, 'min_samples_leaf': 26}
# 0.9482857142857143
# timeElapsed: 104.55771660804749, 2 split

parameters = {'criterion': ['entropy'], 'max_depth': list(range(12, 16, 1)), 'min_samples_leaf': list(range(24, 32, 1))}
# {'criterion': 'entropy', 'max_depth': 13, 'min_samples_leaf': 26}
# 0.9482535714285714
# timeElapsed: 109.73045659065247

# Since we have a lot of parameters and a lot of data, we opted to do a simple holdout instead of a 5-fold or 10-fold cv
startTime = time.time()

gscv = GridSearchCV(DecisionTreeClassifier(), parameters, cv=StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=334))
gscv.fit(xTrain_scaled, yTrain)

endTime = time.time()
timeElapsed = endTime - startTime
print('timeElapsed:', timeElapsed)

# Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html

# Results
print(gscv.cv_results_)

# Best parameter combination
print(gscv.cv_results_['params'][gscv.best_index_])
print(gscv.cv_results_['mean_test_score'][gscv.best_index_])

# Random Forest Classifier
startTime = time.time()

rf = RandomForestClassifier(criterion='entropy', max_depth=15, min_samples_leaf=23, n_estimators=207, max_features=5, oob_score=True, warm_start=True).fit(xTrain_scaled, yTrain)
#rf = RandomForestClassifier(n_estimators=207, oob_score=True, warm_start=True).fit(xTrain_scaled, yTrain)
rf_preds = rf.predict(xTest_scaled)

endTime = time.time()
timeElapsed = endTime - startTime
print(rf_preds)
print(accuracy_score(yTest, rf_preds))
print('timeElapsed:', timeElapsed)

# Code referenced: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

rf_preds_test = rf.predict(xTest_scaled)
rf_preds_train = rf.predict(xTrain_scaled)

print(accuracy_score(yTest, rf_preds_test))
print(accuracy_score(yTrain, rf_preds_train))

# GridSearchCV Random Forest

#parameters = {'criterion': ['gini', 'entropy'], 'max_depth': [5, 10, 15, 20, 25], 'min_samples_leaf': [10, 15, 20, 25, 30], 'max_features': [3, 4, 5, 6, 7], 'n_estimators': [300]}
# criterion: entropy, md: 15, mls: 25, mf: 5

parameters = {'criterion': ['gini', 'entropy'], 'max_depth': [9, 11, 13, 15], 'min_samples_leaf': [21, 23, 25, 27, 29], 'max_features': [3, 5], 'n_estimators': [300]}

startTime = time.time()

gscv = GridSearchCV(RandomForestClassifier(), parameters, cv=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=334))
gscv.fit(xTrain_scaled, yTrain)

endTime = time.time()
timeElapsed = endTime - startTime
print('timeElapsed:', timeElapsed)

# Results
gscv.cv_results_

# Best parameter combination
print(gscv.cv_results_['params'][gscv.best_index_])
print(gscv.cv_results_['mean_test_score'][gscv.best_index_])
print(gscv.cv_results_['mean_test_score'])

# AdaBoosted Decision Tree 
startTime = time.time()

abdt = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=1), n_estimators=150, learning_rate=1.75).fit(xTrain_scaled, yTrain)
abdt_preds = abdt.predict(xTest_scaled)

endTime = time.time()
timeElapsed = endTime - startTime
print(abdt_preds)
print(accuracy_score(yTest, abdt_preds))
print('timeElapsed:', timeElapsed)

# Code reference: https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html

abdt_preds_test = abdt.predict(xTest_scaled)
abdt_preds_train = abdt.predict(xTrain_scaled)

print(accuracy_score(yTest, abdt_preds_test))
print(accuracy_score(yTrain, abdt_preds_train))

# GridSearchCV AdaBoost

#parameters = {'n_estimators': [100, 200, 300, 400], 'learning_rate': [0.75, 1, 1.25, 1.5, 1.75] }
# n_est: 300, lr: 1.75, md: 1

parameters = {'n_estimators': [300], 'learning_rate': [1.7, 1.75, 1.77, 1.8] }

startTime = time.time()

gscv = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=1)), parameters, cv=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=334))
gscv.fit(xTrain_scaled, yTrain)

endTime = time.time()
timeElapsed = endTime - startTime
print('timeElapsed:', timeElapsed)

# Get error with each tree addition
test_errors = []

for pred in abdt.staged_predict(xTest_scaled):
  test_errors.append(1. - accuracy_score(pred, yTest))

n_tree = len(abdt)

plt.plot(range(1, n_tree+1), test_errors)
plt.title('AdaBoost Ensemble')
plt.xlabel('Number of Trees')
plt.ylabel('Error Rate')
plt.show()

# Get OOB training convergence
test_rf = RandomForestClassifier(criterion='entropy', max_depth=15, min_samples_leaf=23, max_features=5, oob_score=True, warm_start=True).fit(xTrain_scaled, yTrain)
test = []

for i in range(100, 300+1):
  test_rf.set_params(n_estimators=i)
  test_rf.fit(xTrain_scaled, yTrain)
  oob_error = 1 - test_rf.oob_score_
  test.append(oob_error)
  print(i)

plt.plot(list(range(100, 301)), test)
plt.title('Random Forest')
plt.xlabel('Number of Trees')
plt.ylabel('OOB Error Rate')
plt.show()

# Check individual accuracies
# Find the indices for each label
print(type(le.inverse_transform([0, 1, 2, 3, 4])))

print(yTest)
anger_labels = [i for i, label in enumerate(yTest) if label == 0]
fear_labels = [i for i, label in enumerate(yTest) if label == 1]
joy_labels = [i for i, label in enumerate(yTest) if label == 2]
no_labels = [i for i, label in enumerate(yTest) if label == 3]
sad_labels = [i for i, label in enumerate(yTest) if label == 4]


print(dt_preds)
print(rf_preds)
print(abdt_preds)

print('anger', accuracy_score(dt_preds[anger_labels], yTest[anger_labels]))
print('fear', accuracy_score(dt_preds[fear_labels], yTest[fear_labels]))
print('joy', accuracy_score(dt_preds[joy_labels], yTest[joy_labels]))
print('no', accuracy_score(dt_preds[no_labels], yTest[no_labels]))
print('sad', accuracy_score(dt_preds[sad_labels], yTest[sad_labels]))

print('anger', accuracy_score(rf_preds[anger_labels], yTest[anger_labels]))
print('fear', accuracy_score(rf_preds[fear_labels], yTest[fear_labels]))
print('joy', accuracy_score(rf_preds[joy_labels], yTest[joy_labels]))
print('no', accuracy_score(rf_preds[no_labels], yTest[no_labels]))
print('sad', accuracy_score(rf_preds[sad_labels], yTest[sad_labels]))

print('anger', accuracy_score(abdt_preds[anger_labels], yTest[anger_labels]))
print('fear', accuracy_score(abdt_preds[fear_labels], yTest[fear_labels]))
print('joy', accuracy_score(abdt_preds[joy_labels], yTest[joy_labels]))
print('no', accuracy_score(abdt_preds[no_labels], yTest[no_labels]))
print('sad', accuracy_score(abdt_preds[sad_labels], yTest[sad_labels]))

# Individual Accuracies
# groups: labels (x-axis, 5)
# vars: model (legend, 3)
dt_anger = accuracy_score(dt_preds[anger_labels], yTest[anger_labels])
dt_fear = accuracy_score(dt_preds[fear_labels], yTest[fear_labels])
dt_joy = accuracy_score(dt_preds[joy_labels], yTest[joy_labels])
dt_no = accuracy_score(dt_preds[no_labels], yTest[no_labels])
dt_sad = accuracy_score(dt_preds[sad_labels], yTest[sad_labels])

rf_anger = accuracy_score(rf_preds[anger_labels], yTest[anger_labels])
rf_fear = accuracy_score(rf_preds[fear_labels], yTest[fear_labels])
rf_joy = accuracy_score(rf_preds[joy_labels], yTest[joy_labels])
rf_no = accuracy_score(rf_preds[no_labels], yTest[no_labels])
rf_sad = accuracy_score(rf_preds[sad_labels], yTest[sad_labels])

abdt_anger = accuracy_score(abdt_preds[anger_labels], yTest[anger_labels])
abdt_fear = accuracy_score(abdt_preds[fear_labels], yTest[fear_labels])
abdt_joy = accuracy_score(abdt_preds[joy_labels], yTest[joy_labels])
abdt_no = accuracy_score(abdt_preds[no_labels], yTest[no_labels])
abdt_sad = accuracy_score(abdt_preds[sad_labels], yTest[sad_labels])

dt_bars = [dt_anger, dt_fear, dt_joy, dt_no, dt_sad]
rf_bars = [rf_anger, rf_fear, rf_joy, rf_no, rf_sad]
abdt_bars = [abdt_anger, abdt_fear, abdt_joy, abdt_no, abdt_sad]

barWidth = 0.25
r1 = np.arange(len(dt_bars))
r2 = [x + barWidth for x in r1]
r3 = [x + barWidth for x in r2]


plt.bar(r1, dt_bars, color='#364F6B', width=barWidth, edgecolor='white', label='Decision Tree')
plt.bar(r2, rf_bars, color='#3FC1C9', width=barWidth, edgecolor='white', label='Random Forest')
plt.bar(r3, abdt_bars, color='#9CC3D5FF', width=barWidth, edgecolor='white', label='AdaBoost DT')

plt.title('Individual Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Emotion Category')
plt.xticks([r + barWidth for r in range(len(dt_bars))], le.inverse_transform([0, 1, 2, 3, 4]))
plt.xticks(rotation=90)

plt.legend(bbox_to_anchor=(1.05, 1))
plt.show()

# Individual Error
# groups: labels (x-axis, 5)
# vars: model (legend, 3)
dt_anger = accuracy_score(dt_preds[anger_labels], yTest[anger_labels])
dt_fear = accuracy_score(dt_preds[fear_labels], yTest[fear_labels])
dt_joy = accuracy_score(dt_preds[joy_labels], yTest[joy_labels])
dt_no = accuracy_score(dt_preds[no_labels], yTest[no_labels])
dt_sad = accuracy_score(dt_preds[sad_labels], yTest[sad_labels])

rf_anger = accuracy_score(rf_preds[anger_labels], yTest[anger_labels])
rf_fear = accuracy_score(rf_preds[fear_labels], yTest[fear_labels])
rf_joy = accuracy_score(rf_preds[joy_labels], yTest[joy_labels])
rf_no = accuracy_score(rf_preds[no_labels], yTest[no_labels])
rf_sad = accuracy_score(rf_preds[sad_labels], yTest[sad_labels])

abdt_anger = accuracy_score(abdt_preds[anger_labels], yTest[anger_labels])
abdt_fear = accuracy_score(abdt_preds[fear_labels], yTest[fear_labels])
abdt_joy = accuracy_score(abdt_preds[joy_labels], yTest[joy_labels])
abdt_no = accuracy_score(abdt_preds[no_labels], yTest[no_labels])
abdt_sad = accuracy_score(abdt_preds[sad_labels], yTest[sad_labels])

dt_bars = [dt_anger, dt_fear, dt_joy, dt_no, dt_sad]
rf_bars = [rf_anger, rf_fear, rf_joy, rf_no, rf_sad]
abdt_bars = [abdt_anger, abdt_fear, abdt_joy, abdt_no, abdt_sad]

# Error
dt_errors = [1 - x for x in dt_bars]
rf_errors = [1 - x for x in rf_bars]
abdt_errors = [1 - x for x in abdt_bars]

barWidth = 0.25
r1 = np.arange(len(dt_errors))
r2 = [x + barWidth for x in r1]
r3 = [x + barWidth for x in r2]


plt.bar(r1, dt_errors, color='#364F6B', width=barWidth, edgecolor='white', label='Decision Tree')
plt.bar(r2, rf_errors, color='#3FC1C9', width=barWidth, edgecolor='white', label='Random Forest')
plt.bar(r3, abdt_errors, color='#9CC3D5FF', width=barWidth, edgecolor='white', label='AdaBoost DT')

plt.title('Individual Error')
plt.ylabel('Error Rate')
plt.xlabel('Emotion Category')
plt.xticks([r + barWidth for r in range(len(dt_errors))], le.inverse_transform([0, 1, 2, 3, 4]))
plt.xticks(rotation=90)

plt.legend(bbox_to_anchor=(1.05, 1))
plt.show()

# Overall Accuracies
dt_acc_test = accuracy_score(dt_preds_test, yTest)
rf_acc_test = accuracy_score(rf_preds_test, yTest)
abdt_acc_test = accuracy_score(abdt_preds_test, yTest)

dt_acc_train = accuracy_score(dt_preds_train, yTrain)
rf_acc_train = accuracy_score(rf_preds_train, yTrain)
abdt_acc_train = accuracy_score(abdt_preds_train, yTrain)

#print(dt_acc, rf_acc, abdt_acc)

train_accs = [dt_acc_train, rf_acc_train, abdt_acc_train]
test_accs = [dt_acc_test, rf_acc_test, abdt_acc_test]

barWidth = 0.25
r1 = np.arange(len(train_accs))
r2 = [x + barWidth for x in r1]

models = ['Decision Tree', 'Random Forest', 'AdaBoost DT']

plt.bar(r1, train_accs, color='red', width=barWidth, edgecolor='white', label='train')
plt.bar(r2, test_accs, color='blue', width=barWidth, edgecolor='white', label='test')

plt.title('Overall Accuracy')
plt.xlabel('Model')
plt.ylabel('Accuracy')

plt.xticks([r + barWidth for r in range(len(train_accs))], models)
plt.xticks(rotation=90)

plt.legend(bbox_to_anchor=(1.05, 1))
plt.show()

# Overall Error
train_errs = [1-dt_acc_train, 1-rf_acc_train, 1-abdt_acc_train]
test_errs = [1-dt_acc_test, 1-rf_acc_test, 1-abdt_acc_test]

barWidth = 0.25
r1 = np.arange(len(train_errs))
r2 = [x + barWidth for x in r1]

models = ['Decision Tree', 'Random Forest', 'AdaBoost DT']

plt.bar(r1, train_errs, color='red', width=barWidth, edgecolor='white', label='train')
plt.bar(r2, test_errs, color='blue', width=barWidth, edgecolor='white', label='test')

plt.title('Overall Error')
plt.xlabel('Model')
plt.ylabel('Error Rate')

plt.xticks([r + barWidth for r in range(len(train_errs))], models)
plt.xticks(rotation=90)

plt.legend(bbox_to_anchor=(1.05, 1))
plt.show()

# Code referenced: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html

from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import label_binarize
from scipy import interp
from itertools import cycle
from sklearn.multiclass import OneVsRestClassifier


yTrain_bin = label_binarize(yTrain, classes=[0, 1, 2, 3, 4])
yTest_bin = label_binarize(yTest, classes=[0, 1, 2, 3, 4])
n_classes = yTrain_bin.shape[1]


#classifier = OneVsRestClassifier(DecisionTreeClassifier(max_depth=13, min_samples_leaf=26, criterion='entropy'))
#y_score = classifier.fit(xTrain_scaled, yTrain_bin).predict_proba(xTest_scaled)

#classifier = OneVsRestClassifier(RandomForestClassifier(criterion='entropy', max_depth=15, min_samples_leaf=23, n_estimators=207, max_features=5, oob_score=True, warm_start=True))
#y_score = classifier.fit(xTrain_scaled, yTrain_bin).predict_proba(xTest_scaled)

#bin_abdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=150, learning_rate=1.75).fit(xTrain_scaled, yTrain_bin)
#y_score = bin_abdt.predict(xTest_scaled)

classifier = OneVsRestClassifier(AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=1), n_estimators=150, learning_rate=1.75))
y_score = classifier.fit(xTrain_scaled, yTrain_bin).predict_proba(xTest_scaled)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(yTest_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

fpr["micro"], tpr["micro"], _ = roc_curve(yTest_bin.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# # Code referenced: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html

all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])

mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

colors = cycle(['#525564', '#74828F', '#96C0CE', '#BEB9B5', '#C25B56'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for AdaBoost DT (multi-class)')
plt.legend(bbox_to_anchor=(1.05, 1))
plt.show()